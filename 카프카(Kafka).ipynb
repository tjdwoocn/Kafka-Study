{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 카프카(Kafka) 란 무엇인가?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 카프카 이전\n",
    "- 데이터를 전송하는 source 어플리케이션과 데이터를 받는 target 어플리케이션이 있었음\n",
    "- 처음에는 간단한 단방향 커뮤니케이션을 수행했었음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ex_screenshot](./kafka_img/screenshot1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ex_screenshot](./kafka_img/screenshot2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문제: \n",
    "    - source/target 어플리케이션의 종류가 많아짐 -> 데이터 전송 라인이 매우 복잡\n",
    "    - 데이터 전송 라인이 많아짐 -> 배포와 대응(respond)가 어려워짐\n",
    "    - 데이터 전송시, 프로토콜/포맷의 파편화가 심각해짐\n",
    "    - 추후 내부 데이터 포맷의 변경사항이 있을때 유지/보수 등이 매우 어려워짐\n",
    "\n",
    "- Apache Kafka(아파치 카프카)는 이런 복잡함을 해결하기 위해 linkedin 에서 개발 및 오픈소스 제공\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ex_screenshot](./kafka_img/screenshot3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apache Kafka\n",
    "    - source 어플리케이션과 target 어플리케이션의 커플링을(내부 복잡도로 이해하면 될듯)\n",
    "      약하게 하기위해 나옴\n",
    "    - source 어플에서 카프카에 데이터 전송\n",
    "    - target 어플은 카프카에서 데이터 가져옴"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ex_screenshot](./kafka_img/screenshot4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Features\n",
    "    - source 어플(Kafka Producer):\n",
    "        - 쇼핑몰의 클릭로그, 결제로그 등을 보냄\n",
    "        - 보내는 데이터의 포맷은 제한이 없음(json, tsv, avro etc)\n",
    "    - target 어플(Kafka Consumer):\n",
    "        - 해당 로그를 적재, 처리 함\n",
    "    - kafka 에는 데이터를 답는 토픽(topic)들이 존재함 (Queue라 생각하면 됨)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ex_screenshot](./kafka_img/screenshot5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kafka 특징\n",
    "    - Fault Tolerant(고가용성): 서버에 문제가 생기거나, 랙(전원이) 내려간다던가<br> 하더라도 데이터 손실 없이 복구 가능\n",
    "    - 낮은 지연(latency), 높은 처리량(Throughput)을 통해 대용량 데이터를 효과적으로 처리 가능<br> -> 빅데이터 처리에 매우 유용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kafka Topic?\n",
    "    - 다양한 데이터가 들어갈 수 있는 공간\n",
    "    - AMQP 와는 다르게 동작(?)\n",
    "    - 데이터베이스의 Table이나 파일시스템의 폴더와 유사한 성질 가짐\n",
    "    - 이름을 가질 수 있음 -> 목적에 따라 어떠한 데이터를 담는지 명확하게 명명\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ex_screenshot](./kafka_img/screenshot6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Topic\n",
    "    - 하나의 토픽은 여러개의 Partition 으로 구성 (위에서 click_log, send_sms 등에 해당)\n",
    "    - Partition은 큐와같이 데이터가 파티션 끝에서부터 순차적으로 쌓임 \n",
    "    - Consumer가 파티션에 붙으면 가장 오래된 데이터부터 가져감\n",
    "    - 더이상의 데이터가 없으면, Consumer는 다른 데이터가 들어올떄까지 대기함\n",
    "    - **Consumer 가 데이터를 가져가더라도 데이터는 삭제되지 않음 **"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ex_screenshot](./kafka_img/screenshot7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Topic\n",
    "    - 새로운 Consumer 가 붙게되면 동일한 데이터를(0번  부터) 가져갈 수 있음\n",
    "    - 다만 Consumer 그룹이 다를경우 auto.offset.reset = earliest 라고 설정되어 있어야 함\n",
    "    - 동일한 데이터에 대해 두번 이상 처리할 수 있는데 이것이 카프카를 사용하는 아주 중요한 이유라고 함\n",
    "    - 클릭로그를 분석/시각하기 위해 Elastic Search에 저장하기도 하고, <br> 클릭로그를 백업하기위해 Hadoop에 저장하기도 함"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ex_screenshot](./kafka_img/screenshot8.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ex_screenshot](./kafka_img/screenshot9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Topic에 Partition 이 2개 이상인 경우\n",
    "    - Partition 이 새로 추가되었고 때마침 새로운 데이터가 들어오는 경우:\n",
    "        - 데이터를 보낼때 키를 지정할 수 있음\n",
    "        - 1) 키가 null 이고(키를 지정하지 않고), 기본 파티션너로 설정하면 round-robin(으로 파티션이 지정\n",
    "        - 2) 키가 있고, 기본 파티션을 사용할경우 -> 키의 해시(hash)값을 구하고 해당 파티션에 할당"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ex_screenshot](./kafka_img/screenshot10.png)\n",
    "\n",
    "* 키를 지정하지 않은경우, 파티션 0 번이 아닌 파티션 1번에 새로운 데이터(7)가 들어감\n",
    "* 이후의 데이터 또한 round-robin으로 파티션 0 과 1에 나뉘어 들어감"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ex_screenshot](./kafka_img/screenshot11.png)\n",
    "<br>\n",
    "- 파티션을 늘릴때는 조심해야함 -> 파티션은 늘릴 수 있지만 줄일 수는 없음!\n",
    "- 그럼에도 늘리는 이유?\n",
    "    - Consumer 의 개수를 늘려 데이터 분산처리가 가능토록 함!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ex_screenshot](./kafka_img/screenshot12.png)\n",
    "<br>\n",
    "- 그렇다면 파티션의 데이터(record)는 언제 삭제되나?\n",
    "    - 옵션에 따라 다름\n",
    "    - 데이터(record) 가 저장되는 최대 보존 시간과 최대 보존 크기를 직접 지정 가능\n",
    "    - 일정한 기간/용량 동안 데이터를 저장 및 삭제 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
